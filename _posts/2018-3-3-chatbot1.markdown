---
layout: post
title: Chatbotåˆæ¢1 qqèŠå¤©æœºå™¨äºº ä¸Š
date: 2018-03-03 16:05:15 +0800
categories: AI
tags: NLP chatbot 
img: http://wangweiguang.xyz/images/chatbot.jpg
---

æƒ³è‡ªå·±åšå‡ ä¸ªèŠå¤©æœºå™¨äººï¼Œå¯ä»¥åœ¨ä¸€äº›å¼€æ”¾çš„è¯é¢˜é‡Œè¿›è¡Œä¸€äº›çŸ­å¯¹è¯ï¼ˆè”ç³»ä¸Šä¸‹æ–‡ä¼°è®¡è¿˜æ˜¯æ¯”è¾ƒéš¾çš„ï¼‰ã€‚

* ç¬¬ä¸€ä¸ªæƒ³åšçš„æ˜¯å¯ä»¥æ¨¡æ‹Ÿä¸€ä¸ªç‰¹å®šçš„äººå¯¹è¯æ¨¡å¼çš„chatbotã€‚æ¯”å¦‚ï¼Œåœ¨qqä¸Šåšä¸€ä¸ªæ¨¡ä»¿ä½ çš„æœºå™¨äººï¼Œå’Œæœ‹å‹è¯´ä¸Šä¸€äºŒåå¥è¯éƒ½çœ‹ä¸å‡ºä»€ä¹ˆç ´ç»½ğŸ˜Šã€‚ï¼ˆå…ˆä»è¿™ä¸ªç»ƒèµ·ï¼‰
* ç¬¬äºŒä¸ªæ˜¯æ¨¡æ‹Ÿå‡ºç‰¹å®šæ€§æ ¼äººçš„å¯¹è¯æ¨¡å¼çš„chatbotï¼Œæ¯”å¦‚è¯´æŒ‰ä¹å‹äººæ ¼ï¼Œæˆ–MBTIçš„äººæ ¼åˆ†ç±»ï¼Œä¸åŒæ€§æ ¼çš„æœºå™¨äººä¼šå¯¹åŒä¸€å¥è¯æœ‰ä¸åŒçš„å›ç­”ã€‚
* ç›®å‰è¿˜æœ‰ä¸€ä¸ªå¤§èƒ†çš„æƒ³æ³•å°±æ˜¯è®©èƒ½ä¸èƒ½è®©æœºå™¨äººä»â€œä¹¦ç±â€é‡Œè¿›è¡Œå­¦ä¹ ï¼Œæ¯”è¾ƒå°å­©æ‰æ˜¯ä¸»è¦ä»æ¨¡ä»¿åˆ«äººå¯¹è¯æ¥å­¦ä¹ å¦‚ä½•å¯¹è¯ï¼Œä½†æˆäººæ›´å¤šæ˜¯é€šè¿‡é˜…è¯»æ¥ä½œä¸ºè¯´è°ˆçš„èµ„æœ¬ã€‚

42

* 
{:toc}

# qqèŠå¤©æœºå™¨äºº
* ç›®æ ‡ï¼šåœ¨qqä¸Šåšä¸€ä¸ªæ¨¡ä»¿ä½ çš„æœºå™¨äººï¼Œå’Œæœ‹å‹è¯´ä¸Šä¸€äºŒåå¥è¯éƒ½çœ‹ä¸å‡ºä»€ä¹ˆç ´ç»½ã€‚
* è¯­æ–™ï¼šqqçš„èŠå¤©è®°å½•å¯ä»¥ç›´æ¥ä½œä¸ºè®­ç»ƒçš„è¯­æ–™ï¼Œè¿™ä¸€ç‚¹æ˜¯å¾ˆæ–¹ä¾¿çš„ã€‚
* æ¨¡å‹ï¼šç›®å‰ä¸ç¡®å®šï¼Œæ…¢æ…¢è¯•è¯•å§ï¼Œç¬¬ä¸€ç¯‡å…ˆä»ç”Ÿæˆå¼çš„ä¸€äº›ç¥ç»ç½‘ç»œæ¨¡å‹å¼€å§‹æ¢ç´¢ä¸€ä¸‹ã€‚
* éš¾ç‚¹ï¼š
  * è¯­æ–™æ–¹é¢ï¼ŒqqèŠå¤©ä¸­å¾€å¾€ä¸æ˜¯â€œä¸€é—®ä¸€ç­”â€çš„ï¼Œè€Œæ˜¯å¤šå¥å¯¹å¤šå¥ï¼›è¯­æ–™ä¸­è¿˜æœ‰å¾ˆå¤šçš„è¡¨æƒ…ç”šè‡³å›¾ç‰‡è§†é¢‘ç­‰å¤šåª’ä½“æ–‡ä»¶ï¼Œä¸æ–¹ä¾¿å¤„ç†ï¼›è¯­æ–™ä¸­å¯èƒ½ä¼šæœ‰ä¸å°‘é”™åˆ«å­—æˆ–æ˜¯è°éŸ³å­—çš„å™ªå£°ï¼ˆæ¯”å¦‚æˆ‘,,ï¼‰ï¼›ä¸ªäººçš„è¯­æ–™å¯èƒ½ä¼šä¼šä¸å¤Ÿå¤šã€‚
  * qqèŠå¤©æœºå™¨äººä½œä¸ºä¸€ç§â€œå¼€æ”¾è¯é¢˜â€å’Œâ€œé•¿å¯¹è¯â€åœºæ™¯çš„æœºå™¨äººï¼Œå¯¹äºç›®å‰çš„å‰æ²¿ç ”ç©¶é¢†åŸŸæ¥è¯´æœ¬èº«å°±è¿˜æ˜¯éš¾é¢˜ã€‚
  * å¦å¤–ï¼Œqqè¿™ä¸ªè½¯ä»¶çš„å¯¹è¯æ¡†ä¸å¥½è·å–èƒ½å¤Ÿè¾“å…¥çš„æ¥å£ï¼Œä¸çŸ¥é“èƒ½ä¸èƒ½è§£å†³ã€‚

ä½œä¸ºèŠå¤©æœºå™¨äººçš„åˆæ¢ï¼Œæˆ‘æ‰¾äº†ä¸€ä¸ªå®Œæ•´çš„é¡¹ç›®è¿›è¡Œäº†å­¦ä¹ ã€‚
è§ï¼š[https://github.com/zhaoyingjun/chatbot](https://github.com/zhaoyingjun/chatbot)

## æ¦‚è¿°
* ç›®æ ‡ï¼šqqèŠå¤©æœºå™¨äººï¼ˆç”Ÿæˆå¼ï¼›çŸ­å¯¹è¯ï¼›å¼€æ”¾è¯é¢˜ï¼‰
* è¯­æ–™ï¼šå’Œå¥³å‹çš„å¯¹è¯é›†ï¼ˆå¤§éƒ¨åˆ†éƒ½æ²¡ä¿å­˜ï¼Œä»…å‰©7ä¸‡å¥ï¼‰
* æ¨¡å‹ï¼šGRUå’ŒLSTMï¼ˆä¾ç…§zhaoyingjunçš„é¡¹ç›®ï¼‰
* æŠ€æœ¯ï¼šseq2seqå’Œword2vec
* å¯è§†åŒ–ï¼šFlask

## æ•°æ®é¢„å¤„ç†
### æ•°æ®å¯¼å…¥
è¯­æ–™å¯ä»¥ç›´æ¥ä»qqä¸­æŠŠèŠå¤©è®°å½•å¯¼å‡ºæ¥ï¼ˆ.txtï¼‰,å…ˆåˆ æ‰[å›¾ç‰‡]ï¼Œ[è¡¨æƒ…],ç­‰è¿™äº›æ„ä¹‰ä¸å¤§çš„æ–‡æœ¬ï¼Œç„¶åè¿›è¡Œè¯»å–å¹¶åˆæ­¥è¿›è¡Œå¤„ç†ï¼ŒæŠŠèŠå¤©è®°å½•åˆ†ä¸ºaskè¯­æ–™é›†å’Œresponseè¯­æ–™é›†ï¼Œå¯¹äºå¤šå¥å¯¹å¤šå¥çš„é—®ç­”ï¼Œè¿™é‡Œç»Ÿä¸€å°†è¿ç»­çš„å¤šå¥åˆå¹¶ä¸ºä¸€å¤§å¥ã€‚

```python
conv_path = "meda.txt"
ask = []  # é—®
response = []  # ç­”
one_conv = ""  # å­˜å‚¨ä¸€æ¬¡å®Œæ•´å¯¹è¯
f = list(open(conv_path, encoding='UTF-8'))
n = 0

for line in f:
    n = n+1 
    line = line.strip('\n')
    if line == '' and flag:
        flag = 0
        continue
    elif ("ä¹ˆå¤§" in line and "-" in line):
        nex = f[n]
        if nex == "\n":
            continue
        if len(one_conv):
            if meda == 0:
                response.append(one_conv)
                one_conv = ""
        meda = 1
    elif "y|c" in line:
        nex = f[n]
        if nex == "\n":
            continue
        if len(one_conv):
            if meda == 1:
                ask.append(one_conv)
                one_conv = ""
        meda = 0
    else:
        if len(one_conv):
            one_conv = one_conv + " " + line
        else:
            one_conv += line
        flag = 1
```
### è¯­å¥åˆ†è¯
```python
ask1 = []      
response1 = [] 
for line in ask:
    line=" ".join(jieba.cut(line))
    ask1.append(line)
for line in response:
    line=" ".join(jieba.cut(line))
    response1.append(line)    
```

### æ•°æ®æ¸…æ´—
è¿™æ ·ä¾èµ–å‰©ä¸‹ä¸‰ä¸‡å¤šå¥å¯¹è¯ï¼Œaské›†å’Œresponseé›†å„ä¸€åŠï¼Œå› ä¸ºå¤šå¥çš„åˆå¹¶ï¼Œæœ‰ä¸€äº›é•¿å¥ï¼Œä¸ºäº†ç®€åŒ–å­¦ä¹ éš¾åº¦ï¼Œå°†é•¿å¥åˆ å»ã€‚ï¼ˆè¿™é‡Œåˆ å»è¶…è¿‡åä¸ªå­—ç¬¦çš„å¥å­ï¼‰

```python
response2 = []
ask2 = []
for n in range (len(ask)):
    if(len(ask[n]) <= 10 and len(response[n]) <=30):
        response2.append(response[n])
        ask2.append(ask[n])
```
### æ•°æ®é›†åˆ’åˆ†
åˆ›å»ºæµ‹è¯•é›†å’Œè®­ç»ƒé›†ï¼Œç›´æ¥æ¬ç”¨å‡½æ•°

```python
def convert_seq2seq_files(questions, answers, TESTSET_SIZE):
    # åˆ›å»ºæ–‡ä»¶
    train_enc = open(gConfig['train_enc'],'w',encoding='UTF-8')  # é—®
    train_dec = open(gConfig['train_dec'],'w',encoding='UTF-8')  # ç­”
    test_enc  = open(gConfig['test_enc'], 'w',encoding='UTF-8')  # é—®
    test_dec  = open(gConfig['test_dec'], 'w',encoding='UTF-8')  # ç­”
  
    test_index = random.sample([i for i in range(len(questions))],TESTSET_SIZE)
 
    for i in range(len(questions)):
        if i in test_index:
            test_enc.write(questions[i]+'\n')
            test_dec.write(answers[i]+ '\n' )
        else:
            train_enc.write(questions[i]+'\n')
            train_dec.write(answers[i]+ '\n' )
        if i % 1000 == 0:
            print(len(range(len(questions))), 'å¤„ç†è¿›åº¦ï¼š', i)
```
ä¸‰åˆ†ä¹‹ä¸€ä½œä¸ºæµ‹è¯•æ ·æœ¬
```python
questions = ask2
answers = response2
convert_seq2seq_files(questions, answers, len(ask1)//3)
```

## æ•°æ®å¤„ç†å™¨
### æ•°æ®å­—å…¸
è¿™ä¸ªå‡½æ•°ç®—æ³•æ€è·¯ä¼šå°†input_fileä¸­çš„å­—ç¬¦å‡ºç°çš„æ¬¡æ•°è¿›è¡Œç»Ÿè®¡ï¼Œå¹¶æŒ‰ç…§ä»å°åˆ°å¤§çš„é¡ºåºæ’åˆ—ï¼Œæ¯ä¸ªå­—ç¬¦å¯¹åº”çš„æ’åºåºå·å°±æ˜¯å®ƒåœ¨è¯å…¸ä¸­çš„ç¼–ç ï¼Œè¿™æ ·å°±å½¢æˆäº†ä¸€ä¸ªkey-vlaueçš„å­—å…¸æŸ¥è¯¢è¡¨ã€‚å‡½æ•°é‡Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µè®¾ç½®å­—å…¸çš„å¤§å°ã€‚
```python
# å®šä¹‰å­—å…¸ç”Ÿæˆå‡½æ•°
def create_vocabulary(input_file,output_file):
    vocabulary = {}
    with open(input_file,'r',encoding='UTF-8') as f:
        counter = 0
        for line in f:
            counter += 1
            tokens = [word for word in line.split()]
            for word in tokens:
                if word in vocabulary:
                    vocabulary[word] += 1
                else:
                    vocabulary[word] = 1
                    vocabulary_list = START_VOCABULART + sorted(vocabulary, key=vocabulary.get, reverse=True)
          # å–å‰20000ä¸ªå¸¸ç”¨æ±‰å­—
        if len(vocabulary_list) > 10000:
            vocabulary_list = vocabulary_list[:10000]
            print(input_file + " è¯æ±‡è¡¨å¤§å°:", len(vocabulary_list))
            with open(output_file, 'w', encoding='UTF-8') as ff:
                for word in vocabulary_list:
                    ff.write(word + "\n")

```
### seq2seq
è¿™ä¸ªå‡½æ•°ä»å‚æ•°ä¸­å°±å¯ä»¥çœ‹å‡ºæ˜¯ç›´æ¥å°†è¾“å…¥æ–‡ä»¶çš„å†…å®¹æŒ‰ç…§è¯å…¸çš„å¯¹åº”å…³ç³»ï¼Œå°†è¯­å¥æ›¿æ¢æˆå‘é‡ï¼Œè¿™ä¹Ÿæ˜¯æ‰€æœ‰seq2seqå¤„ç†çš„æ­¥éª¤ï¼Œå› ä¸ºå®Œæˆè¿™ä¸€æ­¥ä¹‹åï¼Œä¸ç®¡åŸè®­ç»ƒè¯­æ–™æ˜¯ä»€ä¹ˆè¯­è¨€éƒ½æ²¡æœ‰åŒºåˆ«äº†ï¼Œå› ä¸ºå¯¹äºè®­ç»ƒæ¨¡å‹æ¥è¯´éƒ½æ˜¯æ•°å­—åŒ–çš„å‘é‡ã€‚

```python
# æŠŠå¯¹è¯å­—ç¬¦ä¸²è½¬ä¸ºå‘é‡å½¢å¼
def convert_to_vector(input_file, vocabulary_file, output_file):
	print('å¯¹è¯è½¬å‘é‡...')
	tmp_vocab = []
	with open(vocabulary_file, "r",encoding='UTF-8') as f:#è¯»å–å­—å…¸æ–‡ä»¶çš„æ•°æ®ï¼Œç”Ÿæˆä¸€ä¸ªdictï¼Œä¹Ÿå°±æ˜¯é”®å€¼å¯¹çš„å­—å…¸
		tmp_vocab.extend(f.readlines())
	tmp_vocab = [line.strip() for line in tmp_vocab]
	vocab = dict([(x, y) for (y, x) in enumerate(tmp_vocab)])#å°†vocabulary_fileä¸­çš„é”®å€¼å¯¹äº’æ¢ï¼Œå› ä¸ºåœ¨å­—å…¸æ–‡ä»¶é‡Œæ˜¯æŒ‰ç…§{123ï¼šå¥½}è¿™ç§æ ¼å¼å­˜å‚¨çš„ï¼Œæˆ‘ä»¬éœ€è¦æ¢æˆ{å¥½ï¼š123}æ ¼å¼

	output_f = open(output_file, 'w',encoding='UTF-8')
	with open(input_file, 'r',encoding='UTF-8') as f:
		for line in f:
			line_vec = []
			for words in line.split():
				line_vec.append(vocab.get(words, UNK_ID))
			output_f.write(" ".join([str(num) for num in line_vec]) + "\n")#å°†input_fileé‡Œçš„ä¸­æ–‡å­—ç¬¦é€šè¿‡æŸ¥å­—å…¸çš„æ–¹å¼ï¼Œæ›¿æ¢æˆå¯¹åº”çš„keyï¼Œå¹¶ä¿å­˜åœ¨output_file
	output_f.close()
```
### é›†æˆå‡½æ•°
è¿™ä¸ªå‡½æ•°æ˜¯æ•°æ®å¤„ç†å™¨çš„é›†æˆå‡½æ•°ï¼Œæ‰§è¡Œå™¨è°ƒç”¨çš„æ•°æ®å¤„ç†å™¨çš„å‡½æ•°ä¹Ÿä¸»è¦æ˜¯è°ƒç”¨è¿™ä¸ªå‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°æ˜¯å°†é¢„å¤„ç†çš„æ•°æ®ä»ç”Ÿæˆå­—å…¸åˆ°è½¬æ¢æˆå‘é‡ä¸€æ¬¡æ€§æå®šï¼Œå°†æ•°æ®å¤„ç†å™¨å¯¹äºæ‰§è¡Œå™¨æ¥è¯´å®ç°é€æ˜åŒ–ã€‚working_directoryè¿™ä¸ªå‚æ•°æ˜¯å­˜æ”¾è®­ç»ƒæ•°æ®ã€è®­ç»ƒæ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå…¶ä»–å‚æ•°ä¸ä¸€ä¸€ä»‹ç»ã€‚

```python
def prepare_custom_data(working_directory, train_enc, train_dec, test_enc, test_dec, enc_vocabulary_size, dec_vocabulary_size, tokenizer=None):

    # Create vocabularies of the appropriate sizes.
    enc_vocab_path = os.path.join(working_directory, "vocab%d.enc" % enc_vocabulary_size)
    dec_vocab_path = os.path.join(working_directory, "vocab%d.dec" % dec_vocabulary_size)
    
    create_vocabulary(train_enc,enc_vocab_path)
    create_vocabulary(train_dec,dec_vocab_path)
   
    # Create token ids for the training data.
    enc_train_ids_path = train_enc + (".ids%d" % enc_vocabulary_size)
    dec_train_ids_path = train_dec + (".ids%d" % dec_vocabulary_size)
    convert_to_vector(train_enc, enc_vocab_path, enc_train_ids_path)
    convert_to_vector(train_dec, dec_vocab_path, dec_train_ids_path)
 

    # Create token ids for the development data.
    enc_dev_ids_path = test_enc + (".ids%d" % enc_vocabulary_size)
    dec_dev_ids_path = test_dec + (".ids%d" % dec_vocabulary_size)
    convert_to_vector(test_enc, enc_vocab_path, enc_dev_ids_path)
    convert_to_vector(test_dec, dec_vocab_path, dec_dev_ids_path)

    return (enc_train_ids_path, dec_train_ids_path, enc_dev_ids_path, dec_dev_ids_path, enc_vocab_path, dec_vocab_path)
```

## æ‰§è¡Œå™¨
å…·ä½“è®¾ç½®çš„æ—¶å€™ï¼Œæœ‰ä¸¤ä¸ªå¤§åŸåˆ™ï¼šå°½é‡è¦†ç›–åˆ°æ‰€æœ‰çš„è¯­å¥é•¿åº¦ã€æ¯ä¸ªbucketè¦†ç›–çš„è¯­å¥æ•°é‡å°½é‡å‡è¡¡ã€‚ï¼ˆè¿™ä¸ªä¸äº†è§£ï¼Œè¿˜æ²¡æœ‰æŸ¥åˆ°èµ„æ–™ï¼Œæ²¡æœ‰åšä¿®æ”¹ï¼‰
```python
_buckets = [(1, 10), (10, 15), (20, 25), (40, 50)]
```
### æ•°æ®è¯»å–
è¯»å–æµ‹è¯•é›†å’Œè®­ç»ƒé›†

```python
def read_data(source_path, target_path, max_size=None):
    data_set = [[] for _ in _buckets]
    with tf.gfile.GFile(source_path, mode="r") as source_file:
        with tf.gfile.GFile(target_path, mode="r") as target_file:
            source, target = source_file.readline(), target_file.readline()
            counter = 0
            while source and target and (not max_size or counter < max_size):
                counter += 1
                if counter % 100000 == 0:
                    print("  reading data line %d" % counter)
                    sys.stdout.flush()
                source_ids = [int(x) for x in source.split()]
                target_ids = [int(x) for x in target.split()]
                target_ids.append(prepareData.EOS_ID)
                for bucket_id, (source_size, target_size) in enumerate(_buckets):
                    if len(source_ids) < source_size and len(target_ids) < target_size:
                        data_set[bucket_id].append([source_ids, target_ids])
                        break
                    source, target = source_file.readline(), target_file.readline()
    return data_set
```
### æ¨¡å‹å»ºç«‹
ç”Ÿæˆæ¨¡å‹ï¼Œæ¨¡å‹å‚æ•°å…¨éƒ¨å•ç‹¬å­˜å‚¨åœ¨äº†seq2seq.inié‡Œ

```python
def create_model(session, forward_only):

  """Create model and initialize or load parameters"""
  model = seq2seq_model.Seq2SeqModel( gConfig['enc_vocab_size'], gConfig['dec_vocab_size'], _buckets, gConfig['layer_size'], gConfig['num_layers'], gConfig['max_gradient_norm'], gConfig['batch_size'], gConfig['learning_rate'], gConfig['learning_rate_decay_factor'], forward_only=forward_only)

  if 'pretrained_model' in gConfig:
      model.saver.restore(session,gConfig['pretrained_model'])
      return model

  ckpt = tf.train.get_checkpoint_state(gConfig['working_directory'])
  """if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):"""
  if ckpt and ckpt.model_checkpoint_path:
    print("Reading model parameters from %s" % ckpt.model_checkpoint_path)
    model.saver.restore(session, ckpt.model_checkpoint_path)
  else:
    print("Created model with fresh parameters.")
    session.run(tf.global_variables_initializer())
  return model
```
### æ¨¡å‹è®­ç»ƒ
trainå‡½æ•°æ²¡æœ‰å‚æ•°ä¼ é€’ï¼Œå› ä¸ºæ‰€æœ‰çš„å‚æ•°éƒ½æ˜¯é€šè¿‡gconfigæ¥è¯»å–çš„ï¼Œè¿™é‡Œé¢æœ‰ä¸€ä¸ªç‰¹æ®Šçš„è®¾è®¡ï¼Œå°±æ˜¯å°†prepareDataå‡½æ•°è°ƒç”¨æ”¾åœ¨train()å‡½æ•°é‡Œï¼Œè¿™æ ·åšçš„è¯å°±æ˜¯æ¯æ¬¡è¿›è¡Œè®­ç»ƒæ—¶éƒ½ä¼šå¯¹æ•°æ®è¿›è¡Œå¤„ç†ä¸€æ¬¡ï¼Œè¿™ä¸ªå¯ä»¥ä¿è¯æ•°æ®çš„æœ€æ–°ä»¥åŠå¯ä»¥å¯¹å¢é•¿çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™æ˜¯å¾ˆå¥½çš„è®¾è®¡ã€‚

```python
def train():
 # prepare dataset
  print("Preparing data in %s" % gConfig['working_directory'])
  enc_train, dec_train, enc_dev, dec_dev, _, _ = prepareData.prepare_custom_data(gConfig['working_directory'],gConfig['train_enc'],gConfig['train_dec'],gConfig['test_enc'],gConfig['test_dec'],gConfig['enc_vocab_size'],gConfig['dec_vocab_size'])

 
  # setup config to use BFC allocator
  config = tf.ConfigProto()  
  config.gpu_options.allocator_type = 'BFC'
```
### æœ€ç»ˆå¯¹è¯å‡½æ•°
è¿™ä¸ªå‡½æ•°å°±æ˜¯æˆ‘ä»¬æ•´ä¸ªå¯¹è¯æœºå™¨äººçš„æœ€ç»ˆå‡ºæ•ˆæœçš„å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°ä¼šåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå°†è¾“å…¥çš„sentenceè½¬æ¢ä¸ºå‘é‡è¾“å…¥æ¨¡å‹ï¼Œç„¶åå¾—åˆ°æ¨¡å‹çš„ç”Ÿæˆå‘é‡ï¼Œæœ€ç»ˆé€šè¿‡å­—å…¸è½¬æ¢åè¿”å›ç”Ÿæˆçš„è¯­å¥ã€‚

```python
def decode_line(sess, model, enc_vocab, rev_dec_vocab, sentence):
    # Get token-ids for the input sentence.
    token_ids = prepareData.sentence_to_token_ids(tf.compat.as_bytes(sentence), enc_vocab)

    # Which bucket does it belong to?
    bucket_id = min([b for b in xrange(len(_buckets)) if _buckets[b][0] > len(token_ids)])

    # Get a 1-element batch to feed the sentence to the model.
    encoder_inputs, decoder_inputs, target_weights = model.get_batch({bucket_id: [(token_ids, [])]}, bucket_id)

    # Get output logits for the sentence.
    _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)

    # This is a greedy decoder - outputs are just argmaxes of output_logits.
    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]

    # If there is an EOS symbol in outputs, cut them at that point.
    if prepareData.EOS_ID in outputs:
        outputs = outputs[:outputs.index(prepareData.EOS_ID)]

    return " ".join([tf.compat.as_str(rev_dec_vocab[output]) for output in outputs])
```
## å¯è§†åŒ–å±•ç¤ºæ¨¡å—
å¯è§†åŒ–éƒ¨åˆ†ç”¨åˆ°äº†Flaskï¼Œæˆ‘è¿˜ä¸æ˜¯ç‰¹åˆ«ä¼šï¼Œä»¥ååœ¨å¥½å¥½å­¦å§ï¼Œè¿™é‡Œå°±å…ˆç”¨äº†ã€‚

```python
app = Flask(__name__,static_url_path="/static") 

@app.route('/message', methods=['POST'])
def reply():

    req_msg = request.form['msg']
    res_msg = '^_^'
    print(req_msg)
    print(''.join([f+' ' for fh in req_msg for f in fh]))
    req_msg=''.join([f+' ' for fh in req_msg for f in fh])
    print(req_msg)
    res_msg = execute.decode_line(sess, model, enc_vocab, rev_dec_vocab, req_msg )
    
    res_msg = res_msg.replace('_UNK', '^_^')
    res_msg=res_msg.strip()
    
    # å¦‚æœæ¥å—åˆ°çš„å†…å®¹ä¸ºç©ºï¼Œåˆ™ç»™å‡ºç›¸åº”çš„æ¢å¤
    if res_msg == '':
      res_msg = 'ä»€ä¹ˆæ„æ€ï¼Ÿ'

    return jsonify( { 'text': res_msg } )

@app.route("/")
def index(): 
    return render_template("index.html")
```
## æœ€ç»ˆæ•ˆæœå±•ç¤º
### No1 ç²¾åˆ†çš„ä¹ˆå°
ç¬¬ä¸€æ¬¡å‡ºçš„ç»“æœï¼Œç•™ä½œçºªå¿µï¼Œå¯èƒ½ä¸»è¦æ˜¯å› ä¸ºè®­ç»ƒæ—¶é—´æ¯”è¾ƒçŸ­å§ï¼Œå¯¹è¯éƒ½æ²¡æœ‰é€»è¾‘ã€‚æŠŠä»–å«åšç²¾åˆ†ä¹ˆå°ã€‚

![image](http://wangweiguang.xyz/images/jfmx.jpg)

### No2 æ•·è¡çš„ä¹ˆå°
è¿™ä¸ªè®­ç»ƒäº†åä¸ªå°æ—¶ï¼Œè¿­ä»£åˆ°40000æ¬¡ï¼Œç»“æœï¼Œï¼Œï¼Œ

![image](http://wangweiguang.xyz/images/fymx1.jpg)

![image](http://wangweiguang.xyz/images/fymx2.jpg)

![image](http://wangweiguang.xyz/images/fymx3.jpg)

çœ‹èµ·æ¥å¾ˆæœ‰é“ç†çš„æ ·å­ï¼Œæœ‰äº†ç‚¹â€œçµæ€§â€ï¼Œä¸è¿‡åªä¼šå›ç­”ä¸‰å¥è¯ï¼ˆåœ¨qqé‡Œå…¶å®æ˜¯ä¸‰ç§çš„è¡¨æƒ…ï¼‰ï¼Œä¸è¿‡è¿™ä¸‰å¥å€’æ˜¯â€œä¸‡èƒ½å›ç­”â€ï¼Œè¢«AIå­¦ä¹ åˆ°äº†ã€‚

è¿™è¿˜æ˜¯å› ä¸ºæ•°æ®é‡å¤ªå°ï¼ˆ1ä¸‡å¥ï¼‰ï¼Œè®­ç»ƒçš„å¤šäº†æ…¢æ…¢å°±â€œç¨³å®šäº†â€ï¼Œä»¥å‰å°±åšè¿‡ä¸€ä¸ªè¯•éªŒï¼Œè¾“å…¥æ¸¸æˆåç§°å’Œæ¸¸æˆè¯„åˆ†ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œä¹Ÿå°±æ˜¯è®©æœºå™¨å¯»æ‰¾æ¸¸æˆåç§°å’Œæ¸¸æˆè¯„åˆ†çš„å…³ç³»ï¼Œå…¶å®è¿™ç»™çš„ä¿¡æ¯é‡å°±å¤ªå°äº†ï¼Œä¸€ä¸ªæ¸¸æˆï¼Œå…‰ä»åå­—èƒ½çœ‹å‡ºå¤šå°‘ä¸œè¥¿å‘¢ï¼Ÿæ‰€ä»¥åæ¥å‘ç”Ÿçš„æƒ…å†µå°±æ˜¯ï¼Œä¸€å¼€å§‹è®­ç»ƒæœºå™¨ä¼¼ä¹è¿˜åœ¨ç¬¨æ‹™çš„æ‰¾å…³ç³»ï¼Œä¸åŒçš„åç§°ä¼šæœ‰ä¸åŒçš„åˆ†æ•°ï¼Œä½†è®­ç»ƒæ¬¡æ•°å¤šäº†ä»¥åæ…¢æ…¢å°±ç¨³å®šåœ¨äº†ä¸€ä¸ªåˆ†æ•°7åˆ†ï¼Œè€Œç»Ÿè®¡å®é™…çš„è¯„ä»·æ•°æ®ä¸­ä½åˆ†å’Œæé«˜åˆ†å…¶å®éƒ½å¾ˆå°‘ï¼Œä¼—æ•°ä¹Ÿå°±æ˜¯7åˆ†ï¼Œäºæ˜¯æœºå™¨æœ€ç»ˆæ‰¾åˆ°äº†è¿™ä¸ªæœ´ç´ çš„ç­”æ¡ˆã€‚

## æ€»ç»“
è¿™æ¬¡ç®—æ˜¯å®Œæ•´çš„å®Œæˆäº†ä¸€ä¸ªchatbotçš„å°é¡¹ç›®ï¼Œä¸è¿‡è¿™åªæ˜¯ç¬¬ä¸€æ­¥ï¼Œåç»­çš„å·¥ä½œè¿˜æœ‰å¾ˆå¤šã€‚
* å¢åŠ æ•°æ®é›†çš„æ•°é‡å’Œè´¨é‡ï¼Œèµ·ç å¾—20ä¸‡ä»¥ä¸Š
* å¯¹å‚æ•°è¿›è¡Œè°ƒä¼˜ï¼Œä¸è¿‡è¿™ä¸ªåœ¨æˆ‘çš„å°ç¬”è®°æœ¬ä¸Šæ—¶é—´å¼€é”€å¤ªå¤§äº†ï¼Œç­‰åˆ°æŠŠAWSå¼„å¥½äº†ï¼ŒæŠŠæ¨¡å‹æ”¾åˆ°ç½‘ä¸Šè®­ç»ƒï¼Œè¿™ä¸ªè¿‡ç¨‹å°±ä¼šå¿«å¤šäº†ã€‚
* seq2seqï¼Œword2vecï¼ŒLSTMï¼ŒGRUå¯¹è¿™äº›æ–¹æ³•æ¨¡å‹æ›´è¯¦ç»†çš„äº†è§£ï¼Œä¸è¿‡è¿™å°±å¾ˆå¤§äº†ï¼Œå¾ˆå¯èƒ½è¦åˆ°è¯»ç ”æ—¶å†å»æ·±å…¥äº†ï¼Œç°åœ¨å°±ä½œä¸ºæ‹¾é›¶å§
* å¯¹äºè¿™ä¸ªé¡¹ç›®é’ˆå¯¹qqçš„èŠå¤©æœºå™¨äººï¼Œå…¶å®åº”è¯¥åŸºäºç´¢å¼•çš„å®é™…æ•ˆæœä¼šæ›´å¥½ï¼Œæ¥ä¸‹æ¥åœ¨è¯•ä¸€ä¸‹è¿™ç§å®ç°æ–¹æ³•ã€‚