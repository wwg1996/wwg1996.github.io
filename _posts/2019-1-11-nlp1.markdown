---
layout: post
title:  1. NLPåŸºæœ¬æµç¨‹
date: 2019-01-11 14:02:56 +0800
categories: AI
tags: NLP  
img: http://wangweiguang.xyz/images/nlp.jpg
---


* 
{:toc}


æœºå™¨å­¦ä¹ çš„åŸºæœ¬æµç¨‹ä¸»è¦å°±æ˜¯ä¸¤ä¸ªæ­¥éª¤å³

1. æ•°æ®çš„å‡†å¤‡å’Œé¢„å¤„ç†
2. æ¨¡å‹çš„æ„å»ºå’Œè®­ç»ƒ

è€Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä½œä¸ºä¸€ç±»æœºå™¨å­¦ä¹ çš„åº”ç”¨ä¹Ÿå¤åˆè¿™ä¸¤ä¸ªåŸºæœ¬æ­¥éª¤ã€‚è€ŒNLPçš„ç‰¹æ®Šä¹‹å¤„ä¸»è¦åœ¨äºæ•°æ®çš„é¢„å¤„ç†éƒ¨åˆ†ã€‚


![image](http://wangweiguang.xyz/images/nlp1.jpg)


## 0 å‡†å¤‡ï¼šè¯­æ–™ï¼ˆcorpusï¼‰
å½“ä»£è‡ªç„¶è¯­è¨€å¤„ç†éƒ½æ˜¯åŸºäºç»Ÿè®¡çš„ï¼Œç»Ÿè®¡è‡ªç„¶éœ€è¦å¾ˆå¤šæ ·æœ¬ï¼Œå› æ­¤è¯­æ–™å’Œè¯æ±‡èµ„æºæ˜¯å¿…ä¸å¯å°‘çš„ï¼Œæœ¬èŠ‚ä»¥NLTKåº“ä¸ºä¾‹å®‰è£…å’Œä½¿ç”¨è¯­æ–™åº“ã€‚


```python
# NLTKå®‰è£…è¯­æ–™åº“
import nltk
nltk.download()
```

```python
# NLTKâ¾ƒå¸¦è¯­æ–™åº“
>>> from nltk.corpus import brown
>>> brown.categories()
['adventure', 'belles_lettres', 'editorial',
'fiction', 'government', 'hobbies', 'humor',
'learned', 'lore', 'mystery', 'news', 'religion',
'reviews', 'romance', 'science_fiction']
>>> len(brown.sents())
57340
>>> len(brown.words())
1161192
```


```python
# æ”¶é›†è‡ªå·±çš„è¯­æ–™æ–‡ä»¶ï¼ˆæ–‡æœ¬æ–‡ä»¶ï¼‰åˆ°æŸè·¯å¾„ä¸‹ï¼ˆæ¯”å¦‚/tmp)ï¼Œç„¶åæ‰§è¡Œï¼š
>>> from nltk.corpus import PlaintextCorpusReader
>>> corpus_root = '/tmp'
>>> wordlists = PlaintextCorpusReader(corpus_root, '.*')
>>> wordlists.fileids()
# å°±å¯ä»¥åˆ—å‡ºè‡ªå·±è¯­æ–™åº“çš„å„ä¸ªæ–‡ä»¶äº†ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å¦‚wordlists.sents('a.txt')å’Œwordlists.words('a.txt')ç­‰æ–¹æ³•æ¥è·å–å¥å­å’Œè¯ä¿¡æ¯
```

## 1 é¢„å¤„ç†ï¼šåˆ†è¯ï¼ˆtokenizeï¼‰
åˆ†è¯ï¼Œå³æŒ‰ç…§è¯æ³•ï¼ŒæŠŠæ–‡æœ¬åˆ‡æˆä¸€ä¸ªä¸€ä¸ªçš„è¯ã€‚

```python
>>> import nltk
>>> sentence = â€œhello, world"
>>> tokens = nltk.word_tokenize(sentence)
>>> tokens
['hello', â€˜,', 'world']
```

## 1.0 ä¸­æ–‡åˆ†è¯
ä¸­è‹±æ–‡è‡ªç„¶è¯­è¨€å¤„ç†æœ€å¤§çš„åŒºåˆ«åœ¨äºåˆ†è¯ã€‚
```python
import jieba

seg_list =jieba.cut("æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦",cut_all=True)
print("ã€å…¨æ¨¡å¼ã€‘:","/".join(seg_list)) 

seg_list =jieba.cut("æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦",cut_all=False)
print("ã€ç²¾ç¡®æ¨¡å¼ã€‘:","/".join(seg_list)) 

seg_list =jieba.cut("ä»–æ¥åˆ°äº†ç½‘æ˜“æ­ç ”å¤©å¦")
print("ã€æ–°è¯è¯†åˆ«ã€‘:","/".join(seg_list)) 

seg_list =jieba.cut_for_search("å°æ˜ç¡•åœŸæ¯•ä¸šäºä¸­å›½ç§‘å­¦é™¢è®¡ç®—æ‰€ï¼Œååœ¨æ—¥æœ¬äº¬éƒ½å¤§å­¦æ·±é€ ")
print("ã€æ–°è¯è¯†åˆ«ã€‘:","/".join(seg_list)) 
```

```python
ã€å…¨æ¨¡å¼ã€‘: æˆ‘/æ¥åˆ°/åŒ—äº¬/æ¸…å/æ¸…åå¤§å­¦/åå¤§/å¤§å­¦
ã€ç²¾ç¡®æ¨¡å¼ã€‘: æˆ‘/æ¥åˆ°/åŒ—äº¬/æ¸…åå¤§å­¦
ã€æ–°è¯è¯†åˆ«ã€‘: ä»–/æ¥åˆ°/äº†/ç½‘æ˜“/æ­ç ”/å¤©å¦
ã€æ–°è¯è¯†åˆ«ã€‘: å°æ˜/ç¡•åœŸ/æ¯•ä¸š/äº/ä¸­å›½/ç§‘å­¦/å­¦é™¢/ç§‘å­¦é™¢/ä¸­å›½ç§‘å­¦é™¢/è®¡ç®—/è®¡ç®—æ‰€/ï¼Œ/å/åœ¨/æ—¥æœ¬/äº¬éƒ½/å¤§å­¦/æ—¥æœ¬äº¬éƒ½å¤§å­¦/æ·±é€ 
```

## 1.1 å¤„ç†éæ­£å¼çš„ç”¨è¯­
**é—®é¢˜**
â½å¦‚ç¤¾äº¤â½¹ç»œä¸Šï¼Œè¿™äº›ä¹±ä¸ƒâ¼‹ç³Ÿçš„ä¸åˆè¯­æ³•ä¸åˆæ­£å¸¸é€»è¾‘çš„è¯­â¾”å¾ˆå¤šï¼š@æŸâ¼ˆ, è¡¨æƒ…ç¬¦å·, URL, #è¯é¢˜ç¬¦å·ï¼Œå¯¼è‡´æ— æ³•æ­£ç¡®çš„åˆ†è¯ã€‚

**æ–¹æ³•**
æ­£åˆ™è¡¨è¾¾å¼

```python
# nltk.download('punkt')
from nltk.tokenize import word_tokenize

tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'
print(word_tokenize(tweet))

# æ­£åˆ™è¡¨è¾¾å¼å¤„ç† è¡¨æƒ…&ç½‘å€ å­—ç¬¦ä¸²

import re
emoticons_str = r"""
    (?:
        [:=;] #çœ¼ç›
        [oO\-]? #é¼»å­
        [D\)\]\(\]/\\OpP] #å˜´
    )"""
regex_str = [
    emoticons_str,
    r'<[^>]+>', # HTML tags
    r'(?:@[\w_]+)', # @æŸäºº
    r"(?:\#+[\w_]+[\w\'_\-]*[\w_]+)", # è¯é¢˜æ ‡ç­¾
    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs
    r'(?:(?:\d+,?)+(?:\.?\d+)?)', # æ•°å­—
    r"(?:[a-z][a-z'\-_]+[a-z])", # å«æœ‰-å’Œâ€˜çš„å•è¯ å¦‚don't
    r'(?:[\w_]+)', # å…¶ä»–
    r'(?:\S)' # å…¶ä»–
]

tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)
emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)
def tokenize(s):
    return tokens_re.findall(s)

def preprocess(s, lowercase=False):
    tokens = tokenize(s)
    if lowercase:
        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]
    return tokens

tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'
word_list = preprocess(tweet)
print(word_list)
```

## 1.2 å¤„ç†çº·ç¹å¤æ‚çš„è¯å½¢
**é—®é¢˜**

derivationå¼•ç”³: nation (noun) => national (adjective) => nationalize (verb)
å½±å“è¯æ€§

Inflectionå˜åŒ–: walk => walking => walked
ä¸å½±å“è¯æ€§

**æ–¹æ³•**

Stemming è¯å¹²æå–ï¼šâ¼€èˆ¬æ¥è¯´ï¼Œå°±æ˜¯æŠŠä¸å½±å“è¯æ€§çš„inflectionçš„å°å°¾å·´ç æ‰
walking ç ing = walk
walked ç ed = walk

Lemmatization è¯å½¢å½’â¼€ï¼šæŠŠå„ç§ç±»å‹çš„è¯çš„å˜å½¢ï¼Œéƒ½å½’ä¸ºâ¼€ä¸ªå½¢å¼
went å½’â¼€= go
are å½’â¼€= be

### Stemming è¯â¼²æå–
```python
>>> from nltk.stem.porter import PorterStemmer
>>> porter_stemmer = PorterStemmer()
>>> porter_stemmer.stem(â€˜maximumâ€™)
uâ€™maximumâ€™
>>> porter_stemmer.stem(â€˜presumablyâ€™)
uâ€™presumâ€™
>>> porter_stemmer.stem(â€˜multiplyâ€™)
uâ€™multipliâ€™
>>> porter_stemmer.stem(â€˜provisionâ€™)
uâ€™provisâ€™
>>> from nltk.stem import SnowballStemmer
>>> snowball_stemmer = SnowballStemmer(â€œenglishâ€)
>>> snowball_stemmer.stem(â€˜maximumâ€™)
uâ€™maximumâ€™
>>> snowball_stemmer.stem(â€˜presumablyâ€™)
uâ€™presumâ€™
>>> from nltk.stem.lancaster import LancasterStemmer
>>> lancaster_stemmer = LancasterStemmer()
>>> lancaster_stemmer.stem(â€˜maximumâ€™)
â€˜maximâ€™
>>> lancaster_stemmer.stem(â€˜presumablyâ€™)
â€˜presumâ€™
>>> lancaster_stemmer.stem(â€˜presumablyâ€™)
â€˜presumâ€™
>>> from nltk.stem.porter import PorterStemmer
>>> p = PorterStemmer()
>>> p.stem('went')
'went'
>>> p.stem('wenting')
'went'
```

### Lemma è¯å½¢å½’â¼€

```python
>>> from nltk.stem import WordNetLemmatizer
>>> wordnet_lemmatizer = WordNetLemmatizer()
>>> wordnet_lemmatizer.lemmatize(â€˜dogsâ€™)
uâ€™dogâ€™
>>> wordnet_lemmatizer.lemmatize(â€˜churchesâ€™)
uâ€™churchâ€™
>>> wordnet_lemmatizer.lemmatize(â€˜aardwolvesâ€™)
uâ€™aardwolfâ€™
>>> wordnet_lemmatizer.lemmatize(â€˜abaciâ€™)
uâ€™abacusâ€™
>>> wordnet_lemmatizer.lemmatize(â€˜hardrockâ€™)
â€˜hardrockâ€™

```
### æ›´å¥½åœ°å®ç°Lemmaï¼šè¯æ€§æ ‡æ³¨

```python
>>> import nltk
>>> text = nltk.word_tokenize('what does the fox say')
>>> text
['what', 'does', 'the', 'fox', 'say']
>>> nltk.pos_tag(text)
[('what', 'WDT'), ('does', 'VBZ'), ('the', 'DT'), ('fox', 'NNS'), ('say', 'VBP')]

```

## 1.3 å…³äºåœæ­¢è¯ï¼ˆstopwordsï¼‰
åœæ­¢è¯ï¼Œæ˜¯ç”±è‹±æ–‡å•è¯:stopwordç¿»è¯‘è¿‡æ¥çš„ï¼ŒåŸæ¥åœ¨è‹±è¯­é‡Œé¢ä¼šé‡åˆ°å¾ˆå¤šaï¼Œtheï¼Œorç­‰ä½¿ç”¨é¢‘ç‡å¾ˆå¤šçš„å­—æˆ–è¯ï¼Œå¸¸ä¸ºå† è¯ã€ä»‹è¯ã€å‰¯è¯æˆ–è¿è¯ç­‰


```python
from nltk.corpus import stopwords
#å…ˆtokenâ¼€ä¸€æŠŠï¼Œå¾—åˆ°â¼€ä¸€ä¸ªword_list
# ...
#ç„¶åfilterâ¼€ä¸€æŠŠ
filtered_words =
[word for word in word_list if word not in stopwords.words('english')]

```

## 1.4 â¼€æ¡typicalçš„â½‚æœ¬é¢„å¤„ç†æµâ½”çº¿
![image](http://wangweiguang.xyz/images/nlp2.jpg)

## 2 ç‰¹å¾å·¥ç¨‹ï¼ˆFeatureåŒ–ï¼‰
å½“ç»å†å®Œåˆ†è¯çš„è¿‡ç¨‹åï¼Œæ¥ä¸‹æ¥éœ€è¦æŠŠè¯æ±‡è½¬åŒ–ä¸ºè®¡ç®—æœºå¯ä»¥ç†è§£çš„å‘é‡ç»„å½¢å¼ï¼Œå¹¶ä¸”æœ€å¤§é™åº¦åœ°ä»åŸå§‹æ•°æ®ä¸­æå–ç‰¹å¾ä»¥ä¾›ç®—æ³•å’Œæ¨¡å‹ä½¿ç”¨ã€‚ä»¥ä¸‹ä»¥ä¸‰ä¸ªNLPçš„ç»å…¸åº”ç”¨å±•ç°ç‰¹å¾åŒ–çš„è¿‡ç¨‹ã€‚
## 2.1 åº”ç”¨1ï¼šæƒ…æ„Ÿåˆ†æ

### 1. åŸºäºå­—å…¸çš„æ–¹æ³•ï¼ˆsentiment dictionaryï¼‰
like 1
good 2
bad -2
terrible -3
ç±»ä¼¼äºå…³é”®è¯æ‰“åˆ†æœºåˆ¶


```python
words = word_list 

sentiment_dictionary = {}
for line in open('AFINN-111.txt'):
    word, score = line.split('\t')
    sentiment_dictionary[word] = int(score)
#æŠŠè¿™ä¸ªæ‰“åˆ†è¡¨è®°å½•åœ¨ä¸€ä¸ªDictä¸Šä»¥å
#è·‘ä¸€éæ•´ä¸ªå¥ï¤†å­ï¼ŒæŠŠå¯¹åº”çš„å€¼ç›¸åŠ 
total_score = sum(sentiment_dictionary.get(word, 0) for word in words)
#æœ‰å€¼å°±æ˜¯Dictä¸­çš„å€¼ï¼Œæ²¡æœ‰å°±æ˜¯0
#äºæ˜¯ä½ å°±å¾—åˆ°äº†ï¦ºâ¼€ä¸€ä¸ªsentiment score

total_score
```



### 2.  MLæ–¹æ³•

```python
from nltk.classify import NaiveBayesClassifier

#éšâ¼¿æ‰‹é€ ç‚¹è®­ç»ƒé›†
s1 = 'this is a good book'
s2 = 'this is a awesome book'
s3 = 'this is a bad book'
s4 = 'this is a terrible book'

def preprocess(s):
    # Func:ï¤†â¼¦å¤„ï§¤
    #è¿™â¾¥ç®€å•çš„â½¤ï¦ºsplit(),æŠŠå¥å­ä¸­æ¯ä¸ªå•è¯åˆ†å¼€
    #æ˜¾ç„¶è¿˜æœ‰æ›´ï¤å¤šçš„processing methodå¯ä»¥â½¤ç”¨
    return {word: True for word in s.lower().split()}

    # returné•¿è¿™æ ·:
    # {'this': True, 'is':True, 'a':True, 'good':True, 'book':True}
    #å…¶ä¸­,å‰ä¸€ä¸ªå«fname,å¯¹åº”æ¯ä¸ªå‡ºç°çš„â½‚æ–‡æœ¬å•è¯;
    #åä¸€ä¸ªå«fval,æŒ‡çš„æ˜¯æ¯ä¸ªâ½‚æ–‡æœ¬å•è¯å¯¹åº”çš„å€¼ã€‚
    #è¿™â¾¥é‡Œï§©æˆ‘ä»¬â½¤ç”¨æœ€ç®€å•çš„True,æ¥è¡¨ç¤º,è¿™ä¸ªè¯ã€å‡ºç°åœ¨å½“å‰çš„å¥å­ä¸­ã€çš„æ„ä¹‰ã€‚
    #å½“ç„¶å•¦,æˆ‘ä»¬ä»¥åå¯ä»¥å‡çº§è¿™ä¸ªæ–¹ç¨‹,è®©å®ƒå¸¦æœ‰æ›´ï¤åŠ ç‰›é€¼çš„fval,æ¯”å¦‚word2vec

#æŠŠè®­ç»ƒé›†ç»™åšæˆæ ‡å‡†å½¢å¼
training_data = [[preprocess(s1), 'pos'],
                 [preprocess(s2), 'pos'],
                 [preprocess(s3), 'neg'],
                 [preprocess(s4), 'neg']]
#å–‚ç»™modelåƒ
model = NaiveBayesClassifier.train(training_data)
#æ‰“å‡ºç»“æœ
print(model.classify(preprocess('this is a good book')))

```



## 2.2 åº”ç”¨2ï¼šæ–‡æœ¬ç›¸ä¼¼åº¦
ç”¨å…ƒç´ é¢‘ç‡è¡¨ç¤ºâ½‚æœ¬ç‰¹å¾


```python
import nltk
from nltk import FreqDist
#åšä¸ªè¯åº“å…ˆ
corpus = 'this is my sentence ' \
        'this is my life ' \
        'this is the day'
#éšä¾¿tokenizeâ¼€ä¸€ä¸‹
#æ˜¾ç„¶,æ­£å¦‚ä¸Šæ–‡æåˆ°,
#è¿™â¾¥é‡Œï§©å¯ä»¥æ ¹æ®éœ€è¦åšä»»ä½•çš„preprocessing:
# stopwords, lemma, stemming, etc.
tokens = nltk.word_tokenize(corpus)
print(tokens)
#å¾—åˆ°tokenå¥½çš„word list
# ['this', 'is', 'my', 'sentence',
# 'this', 'is', 'my', 'life', 'this',
# 'is', 'the', 'day']
#å€Ÿç”¨NLTKçš„FreqDistç»Ÿè®¡ä¸€ä¸‹â½‚æ–‡å­—å‡ºç°çš„é¢‘ç‡
fdist = FreqDist(tokens)
#å®ƒå°±ç±»ä¼¼äºä¸€ä¸ªDict
#å¸¦ä¸ŠæŸä¸ªå•è¯,å¯ä»¥çœ‹åˆ°å®ƒåœ¨æ•´ä¸ªâ½‚æ–‡ç« ä¸­å‡ºç°çš„æ¬¡æ•°
print(fdist['is'])
# 3

#å¥½,æ­¤åˆ»,æˆ‘ä»¬å¯ä»¥æŠŠæœ€å¸¸â½¤ç”¨çš„50ä¸ªå•è¯æ‹¿å‡ºæ¥
standard_freq_vector = fdist.most_common(50)
size = len(standard_freq_vector)
print(standard_freq_vector)
# [('is', 3), ('this', 3), ('my', 2),
# ('the', 1), ('day', 1), ('sentence', 1),
# ('life', 1)

# Func:æŒ‰ç…§å‡ºç°é¢‘ç‡â¼¤å¤§â¼©å°,è®°å½•ä¸‹æ¯â¼€ä¸€ä¸ªå•è¯çš„ä½ç½®
def position_lookup(v):
    res = {}
    counter = 0
    for word in v:
        res[word[0]] = counter
        counter += 1
        return res
#æŠŠæ ‡å‡†çš„å•è¯ä½ç½®è®°å½•ä¸‹æ¥
standard_position_dict = position_lookup(standard_freq_vector)
print(standard_position_dict)
#å¾—åˆ°ä¸€ä¸ªä½ç½®å¯¹ç…§è¡¨
# {'is': 0, 'the': 3, 'day': 4, 'this': 1,
# 'sentence': 5, 'my': 2, 'life': 6}
```

    ['this', 'is', 'my', 'sentence', 'this', 'is', 'my', 'life', 'this', 'is', 'the', 'day']
    3
    [('this', 3), ('is', 3), ('my', 2), ('sentence', 1), ('life', 1), ('the', 1), ('day', 1)]



```python
import nltk
from nltk import FreqDist
#åšä¸ªè¯åº“å…ˆ
corpus = 'this is my sentence ' \
'this is my life ' \
'this is the day'

tokens = nltk.word_tokenize(corpus)
print(tokens)
```

    ['this', 'is', 'my', 'sentence', 'this', 'is', 'my', 'life', 'this', 'is', 'the', 'day']


## 2.3 åº”ç”¨3ï¼šæ–‡æœ¬åˆ†ç±»
æ— è®ºæ˜¯åˆ†è¾¨æƒ…æ„Ÿï¼Œåˆ†è¾¨æ˜¯å“ªä¸ªä½œè€…å†™çš„éƒ½æ˜¯æ–‡æœ¬åˆ†ç±»ã€‚å¾ˆå¤šçš„NLPé—®é¢˜éƒ½å¯ä»¥è¢«å½’ä¸ºæ–‡æœ¬åˆ†ç±»é—®é¢˜ã€‚

### TF-IDF
**ç‰¹å¾å·¥ç¨‹çš„ç›®çš„å°±æ˜¯åˆ†æå‡ºè¯¥æ–‡æœ¬ä¸­çš„ç‹¬ç‰¹ä¹‹å¤„ï¼Œå‡ºç°å¤ªå°‘çš„è¯è¯­æ— æ³•ä½“ç°è¯¥æ–‡æœ¬çš„ç‰¹ç‚¹ï¼Œå‡ºç°å¤ªå¤šçš„æ–‡æœ¬ä¹Ÿæ— æ³•ä½“ç°æ–‡æœ¬ç‰¹ç‚¹ã€‚**

TF: Term Frequency, è¡¡é‡â¼€ä¸ªtermåœ¨â½‚æ¡£ä¸­å‡ºç°å¾—æœ‰å¤šé¢‘ç¹ã€‚

TF(t) = (tå‡ºç°åœ¨â½‚æ¡£ä¸­çš„æ¬¡æ•°) / (â½‚æ¡£ä¸­çš„termæ€»æ•°).

IDF: Inverse Document Frequency, è¡¡é‡â¼€ä¸ªtermæœ‰å¤šé‡è¦ã€‚

æœ‰äº›è¯å‡ºç°çš„å¾ˆå¤šï¼Œä½†æ˜¯æ˜æ˜¾ä¸æ˜¯å¾ˆæœ‰åµâ½¤ã€‚â½å¦‚â€™is'ï¼Œâ€™theâ€˜ï¼Œâ€™andâ€˜ä¹‹ç±»
çš„ã€‚

ä¸ºäº†å¹³è¡¡ï¼Œæˆ‘ä»¬æŠŠç½•è§çš„è¯çš„é‡è¦æ€§ï¼ˆweightï¼‰æâ¾¼ï¼Œ
æŠŠå¸¸è§è¯çš„é‡è¦æ€§æä½ã€‚

IDF(t) = log_e(â½‚æ¡£æ€»æ•°/ å«æœ‰tçš„â½‚æ¡£æ€»æ•°).

TF-IDF = TF * IDF

ä¸¾ä¸ªæ —â¼¦ğŸŒ°
:
â¼€ä¸ªâ½‚æ¡£æœ‰100ä¸ªå•è¯ï¼Œå…¶ä¸­å•è¯babyå‡ºç°äº†3æ¬¡ã€‚
é‚£ä¹ˆï¼ŒTF(baby) = (3/100) = 0.03.
å¥½ï¼Œç°åœ¨æˆ‘ä»¬å¦‚æœæœ‰10Mçš„â½‚æ¡£ï¼Œbabyå‡ºç°åœ¨å…¶ä¸­çš„1000ä¸ªâ½‚æ¡£ä¸­ã€‚
é‚£ä¹ˆï¼ŒIDF(baby) = log(10,000,000 / 1,000) = 4
æ‰€ä»¥ï¼ŒTF-IDF(baby) = TF(baby) * IDF(baby) = 0.03 * 4 = 0.12



```python
# NLTKå®ç°TF-IDF
from nltk.text import TextCollection
#é¦–å…ˆæŠŠæ‰€æœ‰çš„â½‚æ–‡æ¡£æ”¾åˆ°TextCollectionç±»ä¸­ã€‚
#è¿™ä¸ªç±»ä¼šâ¾ƒè‡ªåŠ¨å¸®ä½ æ–­å¥ï¤†,åšç»Ÿè®¡,åšè®¡ç®—
corpus = TextCollection(['this is sentence one',
                         'this is sentence two',
                         'this is sentence three'])
#ç›´æ¥å°±èƒ½ç®—å‡ºtfidf
# (term:ä¸€å¥ï¤†è¯ä¸­çš„æŸä¸ªterm, text:è¿™ï¤†è¯)
print(corpus.tf_idf('this', 'this is sentence four'))
# 0.444342
#åŒç†ï§¤,æ€ä¹ˆå¾—åˆ°â¼€ä¸ªæ ‡å‡†â¼¤å°çš„vectoræ¥è¡¨ç¤ºæ‰€æœ‰çš„å¥å­?
#å¯¹äºæ¯ä¸ªæ–°å¥ï¤†â¼¦å­
new_sentence = 'this is sentence five'
#éå†â¼€ä¸€éæ‰€æœ‰çš„vocabularyä¸­çš„è¯:
for word in standard_vocab:
    print(corpus.tf_idf(word, new_sentence))
#æˆ‘ä»¬ä¼šå¾—åˆ°â¼€ä¸€ä¸ªå·¨â»“é•¿(=æ‰€æœ‰vocabâ»“é•¿åº¦)çš„å‘é‡
```

## 3 æ¥ä¸‹æ¥å°±æ˜¯MLçš„è¿‡ç¨‹äº†...
å¯èƒ½çš„MLæ¨¡å‹:
SVM
LR
RF
MLP
LSTM
RNN


